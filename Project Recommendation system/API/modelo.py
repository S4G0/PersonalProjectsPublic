# -*- coding: utf-8 -*-
"""Modelo.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1A0T73wvdbHX5Hl1fTiFQzot-XS8vz4jt

#**Análisis y transformación de datos**

##**1. Libraries and data loading**
"""

from statistics import mean
from collections import defaultdict

import numpy as np
import pandas as pd
import math
import json

from sklearn.metrics.pairwise import cosine_similarity
from sklearn.feature_extraction.text import TfidfVectorizer

def generate_recommendations(df_interactions: pd.DataFrame, df_products: pd.DataFrame, df_users: pd.DataFrame, picked_userid: int) -> str:
	
	listas = []

	try:
		"""###**2.1. Colaborative filtering -user based**

		#### **2.1.1. Create User-Product Matrix and normalization**
		In step 2.1.1, we will transform the dataset into a matrix format. The rows of the matrix are products, and the columns of the matrix are stores. The value of the matrix is the product rating of the products if there is a rating. Otherwise, it shows 'NaN'.
		"""

		df_distinct_counts = df_interactions.groupby('user_id')['index'].nunique()
		index=df_distinct_counts[df_distinct_counts>=2].index
		index=index.union([picked_userid])

		# Create user-item matrix
		matrix = df_interactions[df_interactions['user_id'].isin(index)].pivot_table(index='user_id', columns='index', values='nuevo_rating')

		cols_no_nulos = matrix .loc[picked_userid].notna()
		columnas_interes = cols_no_nulos[cols_no_nulos].index

		matrix = matrix[~matrix[columnas_interes].isna().all(axis=1)]
		matrix = matrix .dropna(how='all', axis=1)

		# Obtener las columnas con valores no nulos
		columnas_a_eliminar = matrix.loc[picked_userid].dropna().index

		# Paso 2: Eliminar esas columnas del DataFrame
		aux = matrix.drop(columns=columnas_a_eliminar)

		# Paso 3: Eliminar filas completamente vacías
		aux = aux.dropna(how='all').index

		aux = aux.insert(0, picked_userid)

		matrix = matrix.loc[aux]

		"""#### **2.1.2 Identify Similar Users**

		There are different ways to measure similarities. Pearson correlation and cosine similarity are two widely used methods.

		In this tutorial, we will calculate the user similarity matrix using Pearson correlation.
		"""

		def correlacion_manual(fila, fila_ref):
		    X = fila_ref
		    Y = fila  # Convertimos la fila a array

		    X_mean = np.mean(X)
		    Y_mean = np.mean(Y)

		    numerador = np.sum((X - X_mean) * (Y - Y_mean))
		    denominador = np.sqrt(np.sum((X - X_mean) ** 2)) * np.sqrt(np.sum((Y - Y_mean) ** 2))

		    return numerador / denominador

		user_similarity_total = matrix.apply(lambda fila: correlacion_manual(fila, matrix.loc[picked_userid]), axis=1)

		# Remove picked user ID from the candidate list
		user_similarity_total.drop(index=picked_userid, inplace=True)

		# User similarity threashold
		user_similarity_threshold = 0.6

		# Get top n similar users
		similar_users = user_similarity_total[user_similarity_total>user_similarity_threshold].sort_values(ascending=False)

		"""#### **2.1.3 Narrow Down Item Pool**

		In step 3, we will narrow down the item pool by doing the following:
		1. Remove the products that have been bought by the target user.
		2. Keep only the products that similar users have bought.

		To remove the products bought by the target user, we keep only the row for picked user id in the user-item matrix and remove the items with missing values.
		"""

		# Products that the target user has bought
		picked_userid_bought = matrix[matrix.index == picked_userid].dropna(axis=1, how='all')

		# Products that similar users bought. Remove products that none of the similar users have bought
		similar_user_products = matrix[matrix.index.isin(similar_users.index)].dropna(axis=1, how='all')

		"""Next, we will drop the products that the picked user id watched from the similar user product list. `errors='ignore'` drops columns if they exist without giving an error message."""

		# Remove the watched product from the product list
		similar_user_products.drop(picked_userid_bought.columns,axis=1, inplace=True, errors='ignore')

		"""#### **2.1.4 Recommend Items**

		In step 2.1.4, we will decide which product to recommend to the target user. The recommended items are determined by the weighted average of user similarity score and product rating. The product ratings are weighted by the similarity scores, so the users with higher similarity get higher weights.

		This code loops through items and users to get the item score, rank the score from high to low and pick the top 20 products to recommend to user ID picked.
		"""

		# A dictionary to store item scores
		item_score = {}

		# Loop through items
		for i in similar_user_products.columns:
		  # Get the ratings for product i
		  product_rating = similar_user_products[i]
		  # Create a variable to store the score
		  total = 0
		  # Create a variable to store the number of scores
		  count = 0
		  # Loop through similar users
		  for u in similar_users.index:
		    # If the product has rating
		    if pd.isna(product_rating[u]) == False:
		      # Score is the sum of user similarity score multiply by the product rating
		      score = similar_users[u] * product_rating[u]
		      # Add the score to the total score for the product so far
		      total += score
		      # Add 1 to the count
		      count +=1
		  # Get the average score for the item
		  item_score[i] = total / count

		# Convert dictionary to pandas dataframe
		item_score = pd.DataFrame(item_score.items(), columns=['index', 'Product_score'])

		# Sort the products by score
		ranked_item_score = item_score.sort_values(by='Product_score', ascending=False)
		
		list_3=ranked_item_score["index"].tolist()[:3]
		listas.append(list_3)
	except ValueError:
		print("No se hizo recomendación de filtrado colaborativo.")

	
	try:

		"""###**2.2. Recommender system based in popularity**"""

		n_allowed2 = 50

		# Merge users and interactions, dropping 'tipo_interaccion'
		filtered_df = pd.merge(df_users, df_interactions.drop(['tipo_interaccion'], axis=1), on='user_id', how='inner')

		# Extract user-specific attributes
		picked_user_row = df_users[df_users['user_id'] == picked_userid]

		if not picked_user_row.empty:
			picked_intereses = picked_user_row['intereses'].iloc[0]  # Extract single value
			df_GT_filtered_aux = filtered_df[filtered_df['intereses'] == picked_intereses]

			if df_GT_filtered_aux['user_id'].nunique() > n_allowed2:
				filtered_df = df_GT_filtered_aux

				picked_edad_category = picked_user_row['edad_category'].iloc[0]  # Extract single value
				df_GT_filtered_aux = filtered_df[filtered_df['edad_category'] == picked_edad_category]

				if df_GT_filtered_aux['user_id'].nunique() > n_allowed2:
					filtered_df = df_GT_filtered_aux

					picked_genero = picked_user_row['genero'].values[0]  # Extract single value safely
					if picked_genero in ['Masculino', 'Femenino']:
						df_GT_filtered_aux = filtered_df[filtered_df['genero'] == picked_genero]

						if df_GT_filtered_aux['user_id'].nunique() > n_allowed2:
							filtered_df = df_GT_filtered_aux

		"""We filter the information by intereses, age_category and genre which the picked user belongs, and sort the values by mean_rating in order to visualize the most popular products in the Department and in the Local Segments"""

		# Taking the most popular products of the LS1, LS3, LS4, Department in which the picked store belongs
		agg_ratings_pop = filtered_df.groupby('index').agg(mean_rating = ('nuevo_rating', 'mean'),
				                                number_of_purchases = ('nuevo_rating', 'count')).reset_index()

		# Keep the products with over 10 purchases
		agg_ratings_pop_GT = agg_ratings_pop[agg_ratings_pop['number_of_purchases']>=10]

		"""Finally we remove the products that the store has already bought"""

		most_pop=agg_ratings_pop_GT.drop(agg_ratings_pop_GT[agg_ratings_pop_GT['index'].isin(picked_userid_bought.columns)].index, inplace=False, errors='ignore').sort_values("mean_rating",ascending=False)
		
		list_1=most_pop["index"].tolist()[:3]
		listas.append(list_1)
		
	except ValueError:
		print("No se hizo recomendación basada en popularidad.")
	
	try:

		"""###**2.3. Recommender system with content based filtering**

		Table of ratings for the picked user
		"""

		rt_onlyuser=df_interactions[df_interactions['user_id'] == picked_userid].sort_values(by=['nuevo_rating'], ascending=False).reset_index(drop=True)
		rt_onlyuser = rt_onlyuser.drop('user_id', axis=1)

		"""Create table of products with features"""

		products_w_ft=df_products.groupby(["name", "category", "descripcion", "palabras_clave", 'index']).sum().reset_index().drop(['product_id'], axis=1)

		products_w_ft["Features"] = products_w_ft["category"] + ", "+ products_w_ft["palabras_clave"]+ ","
		products_w_ft["Features"] = products_w_ft["Features"].str.lower()

		"""Vectorize features"""

		tfidf = TfidfVectorizer()
		features_matrix = tfidf.fit_transform(products_w_ft["Features"])

		"""Calculate similarity scores"""

		similarity_scores = cosine_similarity(features_matrix)

		"""Calculate product scores"""

		# get the indices of the products that the user has already seen and rated
		rated_products = rt_onlyuser["index"].to_numpy()

		rated_indices = [products_w_ft[products_w_ft["index"] == p].index[0] for p in rated_products]

		# calculate the mean rating for each product
		mean_ratings = rt_onlyuser.groupby("index")['nuevo_rating'].mean()

		# calculate the weighted mean rating for each product based on similarity scores
		weighted_mean_ratings = np.zeros(len(products_w_ft))

		for i in range(len(products_w_ft)):
		    if i not in rated_indices:
		      similarities = similarity_scores[i, rated_indices]
		      weighted_mean = (similarities * mean_ratings[rated_products]).sum() / np.linalg.norm(mean_ratings[rated_products])
		      weighted_mean_ratings[i] = weighted_mean

		# add the weighted mean ratings to products table
		products_w_ft["weighted_mean_rating"] = weighted_mean_ratings

		# sort the products by weighted mean rating in descending order
		recommended_products = products_w_ft.sort_values("weighted_mean_rating", ascending=False)
		
		list_2=recommended_products["index"].tolist()[:3]
		listas.append(list_2)
		
	except ValueError:
		print("No se hizo recomendación basada en contenido.")

	"""## **3. Final recommendations from all recommenders**"""

	"""**FINAL**"""

	conteo = defaultdict(int)
	indice_minimo = {}

	# Recorrer cada lista y registrar la información
	for lista in listas:
		for pos, num in enumerate(lista):  # Obtener posición en la lista
			conteo[num] += 1
			if num not in indice_minimo:
				indice_minimo[num] = pos  # Guardamos la primera vez que aparece
			else:
				indice_minimo[num] = min(indice_minimo[num], pos)  # Nos aseguramos del índice más bajo

	# Crear el DataFrame con los resultados
	df_resultado = pd.DataFrame({
	    'index': list(conteo.keys()),
	    'Frecuencia': list(conteo.values()),
	    'Indice_Minimo': [indice_minimo[num] for num in conteo.keys()]
	}).sort_values(by=['Frecuencia', 'Indice_Minimo'], ascending=[False, True]).reset_index(drop=True)

	# Mostrar el DataFrame
	df_final = pd.merge(df_resultado[:3][['index']], df_products.groupby(['name', 'category', 'index'], as_index=False).agg({'product_id': 'first'}), on='index', how='left')


	# Transform the DataFrame into a list of dictionaries in the desired format
	recommendations = [
	    {"product_id": str(row["product_id"]), "name": row["name"], "category": row["category"]}
	    for _, row in df_final.iterrows()
	]

	# Create the final JSON structure
	recommendations_json = {"user_id": picked_userid, "recommendations": recommendations}

	# Show the JSON output
	json_output = json.dumps(recommendations_json, ensure_ascii=False)

	return json_output
